{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To train a variable misuse detection model one needs to implement an NLP labeling model.\n",
    "\n",
    "For example, for a funciton containing misuse\n",
    "```\n",
    "def _eq(l1, l2):\\n    return (set(l1) == set(l1))\n",
    "```\n",
    "the misuse character span is (44, 46). To do this with NLP methods, code is tokenized, and labels for tokens are generated\n",
    "```\n",
    "[def, _, eq, (, l, 1, \",\", l, 2, ):, \\n, \\t, return, (, set, (, l1, ), ==, set, (, l1, ), ), ]\n",
    "[O  , O, O , O, O, O,  O , O, O, 0 , O , O ,    O  , O, O  , O, O , O, O , O  , O, M , O, O, O\n",
    "```\n",
    "The goal is to train an NLP model that predicts those labels correctly. In this project, BILUO labeling scheme is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The goal of this project\n",
    "1. Verify dataset, make sure that encoded batches are correct (misuse spans are correct). You can sample dataset and make sure that the number of errors is less than a certain threshold.\n",
    "2. Train variable misuse detection model (with finetuning and without)\n",
    "3. Verify [scoring function](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L71)\n",
    "4. Conduct a series of experiments to identify performance\n",
    "5. Analyze errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Why using this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Basic functionality, necessary for train an NLP labeler is\n",
    "1. Loading data (implemented in this example)\n",
    "2. Tokenization, preparing labels (implemented in [`PythonBatcher.prepare_sent`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L123))\n",
    "3. Data encoding for using with ML models (implemented in [`PythonBatcher.create_batches_with_mask`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L206))\n",
    "4. Batching (implemented in [`PythonBatcher.format_batch`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/batchers/PythonBatcher.py#L256))\n",
    "5. Model training (partially implemented in [`CodeBertModelTrainer2.train_model`](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/codebert/codebert_train.py#L148) and extended here)\n",
    "6. Tensorboard tracking (implemented in `CodeBertModelTrainer2`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. See [installation steps](https://github.com/VitalyRomanov/method-embedding#installing-python-libraries).\n",
    "\n",
    "2. Install transformers\n",
    "```bash\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 04:44:42.342592: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-29 04:44:42.342611: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from argparse import Namespace\n",
    "from SourceCodeTools.nlp.codebert.codebert_train import HybridModelTrainer, test_step, train_step_finetune, CodeGPT2HybridModel, batch_to_torch\n",
    "from SourceCodeTools.nlp.entity.type_prediction import scorer\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from time import time\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "from typing import Dict, Optional, List, Union\n",
    "from SourceCodeTools.nlp.tokenizers import _inject_tokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import diskcache as dc\n",
    "\n",
    "\n",
    "from SourceCodeTools.nlp.entity import fix_incorrect_tags\n",
    "from SourceCodeTools.code.annotator_utils import adjust_offsets, biluo_tags_from_offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "Current code works with many tokenizers. The most comparible format for storing labels is to store them as character spans. Character spans for labels are mapped to tokens with Spacy's `biluo_tags_from_offsets`. For this reason, we need to have instruments to make tokenizers compatible with Spacy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AdapterDoc:\n",
    "    \"\"\"\n",
    "    A simple wrapper for tokens that also stores additional data such as character span adjustment and \n",
    "    tokens compatible with `biluo_tags_from_offsets`\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.adjustment_amount = 0\n",
    "        self.tokens_for_biluo_alignment = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tokens)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\".join(self.tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "class ModelAdapter:\n",
    "    def __init__(self, primary_tokenization):\n",
    "        self.primary_tokenization = primary_tokenization\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "        self.regex_tok = create_tokenizer(\"regex\")\n",
    "\n",
    "    def secondary_tokenization(self, tokens):\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens.extend(self.regex_tok(token))\n",
    "        return new_tokens\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = self.primary_tokenization(text)\n",
    "        tokens = self.secondary_tokenization(tokens)\n",
    "        doc = Doc(self.nlp.vocab, tokens, spaces=[False] * len(tokens))\n",
    "\n",
    "        backup_tokens = doc\n",
    "        fixed_spaces = [False]\n",
    "        fixed_words = [\"<s>\"]\n",
    "\n",
    "        for ind, t in enumerate(doc):\n",
    "            if len(t.text) > 1:\n",
    "                fixed_words.append(t.text.strip(\"Ġ\"))\n",
    "            else:\n",
    "                fixed_words.append(t.text)\n",
    "            if ind != 0:\n",
    "                fixed_spaces.append(t.text.startswith(\"Ġ\") and len(t.text) > 1)\n",
    "        fixed_spaces.append(False)\n",
    "        fixed_spaces.append(False)\n",
    "        fixed_words.append(\"</s>\")\n",
    "\n",
    "        assert len(fixed_spaces) == len(fixed_words)\n",
    "\n",
    "        doc = Doc(self.nlp.vocab, fixed_words, fixed_spaces)\n",
    "\n",
    "        assert len(doc) - 2 == len(backup_tokens)\n",
    "        assert len(doc.text) - 7 == len(backup_tokens.text)\n",
    "\n",
    "        final_doc = AdapterDoc([\"<s>\"] + [t.text for t in backup_tokens] + [\"</s>\"])\n",
    "        final_doc.adjustment_amount = -3\n",
    "        final_doc.tokens_for_biluo_alignment = doc\n",
    "\n",
    "        return final_doc\n",
    "    \n",
    "    \n",
    "def create_tokenizer(type, bpe_path=None, regex=None):\n",
    "    if type == \"spacy\":\n",
    "        import spacy\n",
    "        print(\"Creating spacy tokenizer\")\n",
    "        return _inject_tokenizer(spacy.blank(\"en\"))\n",
    "    elif type == \"codebert\":\n",
    "        from transformers import RobertaTokenizer\n",
    "        import spacy\n",
    "        from spacy.tokens import Doc\n",
    "\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        adapter = ModelAdapter(primary_tokenization=tokenizer.tokenize)\n",
    "\n",
    "        def tokenize(text):\n",
    "            return adapter(text)\n",
    "\n",
    "        return tokenize\n",
    "    elif type == \"codegpt2\":\n",
    "        print('came here')\n",
    "        from transformers import GPT2Tokenizer\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"microsoft/CodeGPT-small-py\")\n",
    "        adapter = ModelAdapter(primary_tokenization=tokenizer.tokenize)\n",
    "\n",
    "        def tokenize(text):\n",
    "            return adapter(text)\n",
    "        \n",
    "        return tokenize\n",
    "    else:\n",
    "        raise Exception(\"Supported tokenizer types: spacy, regex, bpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, List, Union\n",
    "from SourceCodeTools.nlp import create_tokenizer, tag_map_from_sentences, TagMap, token_hasher, try_int, ValueEncoder\n",
    "\n",
    "\n",
    "class SampleEntry(object):\n",
    "    def __init__(self, id, text, labels=None, category=None, **kwargs):\n",
    "        self._storage = dict()\n",
    "        self._storage[\"id\"] = id\n",
    "        self._storage[\"text\"] = text\n",
    "        self._storage[\"labels\"] = labels\n",
    "        self._storage[\"category\"] = category\n",
    "        self._storage.update(kwargs)\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        storage = object.__getattribute__(self, \"_storage\")\n",
    "        if item in storage:\n",
    "            return storage[item]\n",
    "        return super().__getattribute__(item)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._storage)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._storage[item]\n",
    "\n",
    "    def keys(self):\n",
    "        return list(self._storage.keys())\n",
    "\n",
    "\n",
    "class MapperSpec:\n",
    "    def __init__(self, field, target_field, encoder, dtype=np.int32, preproc_fn=None):\n",
    "        self.field = field\n",
    "        self.target_field = target_field\n",
    "        self.encoder = encoder\n",
    "        self.preproc_fn = preproc_fn\n",
    "        self.dtype = dtype\n",
    "\n",
    "        \n",
    "class SimplePythonBatcher:\n",
    "    def __init__(\n",
    "            self, data, batch_size: int, seq_len: int,\n",
    "            wordmap: Dict[str, int], *, tagmap: Optional[TagMap] = None,\n",
    "            class_weights=False, element_hash_size=1000, sort_by_length=True, tokenizer=\"spacy\", no_localization=False,\n",
    "            cache_dir: Optional[Union[str, Path]] = None, **kwargs\n",
    "    ):\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._max_seq_len = seq_len\n",
    "        self._tokenizer = tokenizer\n",
    "        self._class_weights = None\n",
    "        self._no_localization = no_localization\n",
    "        self._nlp = create_tokenizer(tokenizer)\n",
    "        self._cache_dir = Path(cache_dir) if cache_dir is not None else cache_dir\n",
    "        self._valid_sentences = 0\n",
    "        self._filtered_sentences = 0\n",
    "        self._wordmap = wordmap\n",
    "        self.tagmap = tagmap\n",
    "        self._sort_by_length = sort_by_length\n",
    "\n",
    "        self._create_cache()\n",
    "        self._prepare_data(data)\n",
    "        self._create_mappers(**kwargs)\n",
    "\n",
    "    def _get_version_code(self):\n",
    "        defining_parameters = json.dumps({\n",
    "            \"tokenizer\": self._tokenizer, \"max_seq_len\": self._max_seq_len\n",
    "        })\n",
    "        return self._compute_text_id(defining_parameters)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_text_id(text):\n",
    "        return int(hashlib.md5(text.encode('utf-8')).hexdigest(), 16) % 1152921504606846976\n",
    "\n",
    "    def _check_cache_dir(self):\n",
    "        if not hasattr(self, \"_cache_dir\") or self._cache_dir is None:\n",
    "            raise Exception(\"Cache directory location has not been specified yet\")\n",
    "\n",
    "    def _get_cache_location_name(self, cache_name):\n",
    "        self._check_cache_dir()\n",
    "        return str(self._cache_dir.joinpath(cache_name))\n",
    "\n",
    "    @property\n",
    "    def _data_cache_path(self):\n",
    "        return self._get_cache_location_name(\"DataCache\")\n",
    "\n",
    "    # @property\n",
    "    # def _sent_cache_path(self):\n",
    "    #     return self._get_cache_location_name(\"SentCache\")\n",
    "\n",
    "    @property\n",
    "    def _batch_cache_path(self):\n",
    "        return self._get_cache_location_name(\"BatchCache\")\n",
    "\n",
    "    @property\n",
    "    def _tagmap_path(self):\n",
    "        return self._cache_dir.joinpath(\"tagmap.json\")\n",
    "\n",
    "    def _create_cache(self):\n",
    "        if self._cache_dir is None:\n",
    "            self._tmp_dir = tempfile.TemporaryDirectory()\n",
    "            self._cache_dir = Path(self._tmp_dir.name)\n",
    "\n",
    "        self._cache_dir = self._cache_dir.joinpath(f\"PythonBatcher{self._get_version_code()}\")\n",
    "        self._cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Created cache directory at location {self._cache_dir}\")\n",
    "\n",
    "        self._data_cache = dict()  #dc.Cache(self._data_cache_path)\n",
    "        # self._sent_cache = dc.Cache(self._sent_cache_path)\n",
    "        self._batch_cache = dc.Cache(self._batch_cache_path)\n",
    "\n",
    "    def _prepare_data(self, data):\n",
    "        self._sent_lenghts = {}\n",
    "\n",
    "        for text, annotations in tqdm(data, desc=\"Preprocess functions\"):\n",
    "            id_ = self._compute_text_id(text)\n",
    "            if id_ not in self._data_cache:\n",
    "                extra = copy(annotations)\n",
    "                labels = extra.pop(\"entities\")\n",
    "                extra.update(self._prepare_tokenized_sent((text, annotations)))\n",
    "                entry = SampleEntry(id=id_, text=text, labels=labels, **extra)\n",
    "                self._data_cache[id_] = entry\n",
    "            else:\n",
    "                entry = self._data_cache[id_]\n",
    "            self._sent_lenghts[id_] = len(entry.tokens)\n",
    "        \n",
    "    def _iterate_record_ids(self):\n",
    "        return self._data_cache.iterkeys()\n",
    "    \n",
    "    def _get_record_with_id(self, id):\n",
    "        if id not in self._data_cache:\n",
    "            raise KeyError(\"Record with such id is not found\")\n",
    "        return self._data_cache[id]\n",
    "\n",
    "    def _iterate_sorted_by_length(self, limit_max_length=False):\n",
    "        for id_, length in sorted(self._sent_lenghts.items(), key=lambda x: x[1]):\n",
    "            if limit_max_length and length >= self._max_seq_len:\n",
    "                continue\n",
    "            yield self._get_record_with_id(id_)\n",
    "\n",
    "    def _iterate_records(self, limit_max_length=False, shuffle=False):\n",
    "        for id_ in self._sent_lenghts.keys():\n",
    "            if limit_max_length and self._sent_lenghts[id_] >= self._max_seq_len:\n",
    "                continue\n",
    "            yield self._get_record_with_id(id_)\n",
    "\n",
    "    def _create_mappers(self, **kwargs):\n",
    "        self._mappers = []\n",
    "        self._create_wordmap_encoder()\n",
    "        self._create_tagmap_encoder()\n",
    "\n",
    "    def _create_tagmap_encoder(self):\n",
    "        if self.tagmap is None:\n",
    "            if self._tagmap_path.is_file():\n",
    "                tagmap = TagMap.load(self._tagmap_path)\n",
    "            else:\n",
    "                def iterate_tags():\n",
    "                    for record in self._iterate_records():\n",
    "                        for label in record.tags:\n",
    "                            yield label\n",
    "\n",
    "                tagmap = tag_map_from_sentences(iterate_tags())\n",
    "                tagmap.set_default(tagmap._value_to_code[\"O\"])\n",
    "                tagmap.save(self._tagmap_path)\n",
    "\n",
    "            self.tagmap = tagmap\n",
    "\n",
    "        self._mappers.append(\n",
    "            MapperSpec(field=\"tags\", target_field=\"tags\", encoder=self.tagmap)\n",
    "        )\n",
    "        # self.tagmap = tagmap\n",
    "        # self.tagpad = self.tagmap[\"O\"]\n",
    "\n",
    "    def _create_wordmap_encoder(self):\n",
    "        wordmap_enc = ValueEncoder(value_to_code=self._wordmap)\n",
    "        wordmap_enc.set_default(len(self._wordmap))\n",
    "        self._mappers.append(\n",
    "            MapperSpec(field=\"tokens\", target_field=\"tok_ids\", encoder=wordmap_enc)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(self.tagmap)\n",
    "\n",
    "    def _prepare_tokenized_sent(self, sent):\n",
    "        text, annotations = sent\n",
    "\n",
    "        doc = self._nlp(text)\n",
    "        ents = annotations['entities']\n",
    "\n",
    "        tokens = doc\n",
    "        try:\n",
    "            tokens = [t.text for t in tokens]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if hasattr(doc, \"tokens_for_biluo_alignment\"):\n",
    "            entity_adjustment_amount = doc.adjustment_amount\n",
    "            tokens_for_biluo_alignment = doc.tokens_for_biluo_alignment\n",
    "        else:\n",
    "            entity_adjustment_amount = 0\n",
    "            tokens_for_biluo_alignment = doc\n",
    "\n",
    "        ents_tags = biluo_tags_from_offsets(\n",
    "            tokens_for_biluo_alignment, adjust_offsets(ents, entity_adjustment_amount),\n",
    "            self._no_localization\n",
    "        )\n",
    "        fix_incorrect_tags(ents_tags)\n",
    "\n",
    "        assert len(tokens) == len(ents_tags)\n",
    "\n",
    "        output = {\n",
    "            \"tokens\": tokens,\n",
    "            \"tags\": ents_tags\n",
    "        }\n",
    "\n",
    "        return output\n",
    "\n",
    "    # @lru_cache(maxsize=200000)\n",
    "    def _encode_for_batch(self, record):\n",
    "\n",
    "        if record.id in self._batch_cache:\n",
    "            return self._batch_cache[record.id]\n",
    "\n",
    "        def encode(seq, encoder, pad, preproc_fn=None):\n",
    "            if preproc_fn is None:\n",
    "                def preproc_fn(x):\n",
    "                    return x\n",
    "            blank = np.ones((self._max_seq_len,), dtype=np.int32) * pad\n",
    "            encoded = np.array([encoder[preproc_fn(w)] for w in seq], dtype=np.int32)\n",
    "            blank[0:min(encoded.size, self._max_seq_len)] = encoded[0:min(encoded.size, self._max_seq_len)]\n",
    "            return blank\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        for mapper in self._mappers:\n",
    "            output[mapper.target_field] = encode(\n",
    "                seq=record[mapper.field], encoder=mapper.encoder, pad=mapper.encoder.default,\n",
    "                preproc_fn=mapper.preproc_fn\n",
    "            ).astype(mapper.dtype)\n",
    "\n",
    "        tokens = record.tokens\n",
    "        num_tokens = len(tokens)\n",
    "\n",
    "        # assert len(s) == len(t)\n",
    "\n",
    "        output[\"no_loc_mask\"] = np.array([tag != self.tagmap.default for tag in output[\"tags\"]]).astype(np.bool)\n",
    "        output[\"lens\"] = num_tokens if num_tokens < self._max_seq_len else self._max_seq_len\n",
    "\n",
    "        self._batch_cache[record.id] = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def format_batch(self, batch):\n",
    "        fbatch = defaultdict(list)\n",
    "\n",
    "        for sent in batch:\n",
    "            for key, val in sent.items():\n",
    "                fbatch[key].append(val)\n",
    "\n",
    "        max_len = max(fbatch[\"lens\"])\n",
    "\n",
    "        return {\n",
    "            key: np.stack(val)[:,:max_len] if key != \"lens\" and key != \"replacements\" and key != \"tokens\"\n",
    "            else (np.array(val, dtype=np.int32) if key == \"lens\" else np.array(val)) for key, val in fbatch.items()}\n",
    "\n",
    "    def generate_batches(self):\n",
    "        batch = []\n",
    "        if self._sort_by_length:\n",
    "            records = self._iterate_sorted_by_length(limit_max_length=True)\n",
    "        else:\n",
    "            records = self._iterate_records(limit_max_length=True, shuffle=False)\n",
    "\n",
    "        for sent in records:\n",
    "            batch.append(self._encode_for_batch(sent))\n",
    "            if len(batch) >= self._batch_size:\n",
    "                yield self.format_batch(batch)\n",
    "                batch = []\n",
    "        if len(batch) > 0:\n",
    "            yield self.format_batch(batch)\n",
    "        # yield self.format_batch(batch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        total_valid = sum(1 for id_, length in self._sent_lenghts.items() if length < self._max_seq_len)\n",
    "        return int(ceil(total_valid / self._batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(dataset_path, partition):\n",
    "    \"\"\"\n",
    "    Read data storead as JSON records.\n",
    "    \"\"\"\n",
    "    assert partition in {\"train\", \"val\", \"test\"}\n",
    "    data_path = join(dataset_path, f\"var_misuse_seq_{partition}.json\")\n",
    "    \n",
    "    data = []\n",
    "    for line in open(data_path, \"r\"):\n",
    "        entry = json.loads(line)\n",
    "        \n",
    "        text = entry.pop(\"text\")\n",
    "        data.append((text, entry))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariableMisuseDetector(HybridModelTrainer):\n",
    "    def set_batcher_class(self):\n",
    "        self.batcher = SimplePythonBatcher\n",
    "        \n",
    "    def get_batcher(self, *args, **kwargs):\n",
    "        kwargs.update({\"tokenizer\": self.model_name})\n",
    "        return self.batcher(*args, **kwargs)\n",
    "\n",
    "    def get_trial_dir(self):\n",
    "        \"\"\"\n",
    "        Define folder name format for storing checkpoints.\n",
    "        \"\"\"\n",
    "        return os.path.join(self.output_dir, f\"{self.model_name}_var_mususe\" + str(datetime.now())).replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "    \n",
    "    def train(\n",
    "            self, model, train_batches, test_batches, epochs, report_every=10, scorer=None, learning_rate=0.01,\n",
    "            learning_rate_decay=1., finetune=False, summary_writer=None, save_ckpt_fn=None, no_localization=False\n",
    "    ):\n",
    "        # all training options are specified [here](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L256)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=learning_rate_decay)  # there is no learning rate decay by default\n",
    "\n",
    "        # metric history is stored here\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_f1s = []\n",
    "        test_f1s = []\n",
    "\n",
    "        num_train_batches = len(train_batches)\n",
    "        num_test_batches = len(test_batches)\n",
    "\n",
    "        best_f1 = 0.\n",
    "\n",
    "        for e in range(epochs):\n",
    "            losses = []\n",
    "            ps = []\n",
    "            rs = []\n",
    "            f1s = []\n",
    "\n",
    "            start = time()\n",
    "            model.train()\n",
    "\n",
    "            for ind, batch in enumerate(tqdm(train_batches)):\n",
    "                batch_to_torch(batch, self.device)  # inspect the content of `batch`\n",
    "                loss, p, r, f1 = train_step_finetune(\n",
    "                    model=model, optimizer=optimizer, token_ids=batch['tok_ids'],\n",
    "                    prefix=None, suffix=None, graph_ids=None,  # keep this None\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=None,  # Keep this None\n",
    "                    scorer=scorer,\n",
    "                    finetune=finetune and e / epochs > 0.2,  # finetuning starts after 20% of training is complete\n",
    "                    vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "                losses.append(loss.cpu().item())\n",
    "                ps.append(p)\n",
    "                rs.append(r)\n",
    "                f1s.append(f1)\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Train\", loss, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Train\", p, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Train\", r, global_step=e * num_train_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Train\", f1, global_step=e * num_train_batches + ind)\n",
    "\n",
    "            test_alosses = []\n",
    "            test_aps = []\n",
    "            test_ars = []\n",
    "            test_af1s = []\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for ind, batch in enumerate(test_batches):\n",
    "                batch_to_torch(batch, self.device)\n",
    "                \n",
    "                test_loss, test_p, test_r, test_f1 = test_step(\n",
    "                    model=model, token_ids=batch['tok_ids'],\n",
    "                    prefix=None, suffix=None, graph_ids=None,  # keep this None\n",
    "                    labels=batch['tags'], lengths=batch['lens'],\n",
    "                    extra_mask=None,  # keep this None\n",
    "                    scorer=scorer, vocab_mapping=self.vocab_mapping\n",
    "                )\n",
    "\n",
    "                self.summary_writer.add_scalar(\"Loss/Test\", test_loss, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Precision/Test\", test_p, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"Recall/Test\", test_r, global_step=e * num_test_batches + ind)\n",
    "                self.summary_writer.add_scalar(\"F1/Test\", test_f1, global_step=e * num_test_batches + ind)\n",
    "                test_alosses.append(test_loss.cpu().item())\n",
    "                test_aps.append(test_p)\n",
    "                test_ars.append(test_r)\n",
    "                test_af1s.append(test_f1)\n",
    "\n",
    "            epoch_time = time() - start\n",
    "\n",
    "            train_losses.append(float(sum(losses) / len(losses)))\n",
    "            train_f1s.append(float(sum(f1s) / len(f1s)))\n",
    "            test_losses.append(float(sum(test_alosses) / len(test_alosses)))\n",
    "            test_f1s.append(float(sum(test_af1s) / len(test_af1s)))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {e}, {epoch_time: .2f} s, Train Loss: {train_losses[-1]: .4f}, Train P: {sum(ps) / len(ps): .4f}, Train R: {sum(rs) / len(rs): .4f}, Train F1: {sum(f1s) / len(f1s): .4f}, \"\n",
    "                f\"Test loss: {test_losses[-1]: .4f}, Test P: {sum(test_aps) / len(test_aps): .4f}, Test R: {sum(test_ars) / len(test_ars): .4f}, Test F1: {test_f1s[-1]: .4f}\")\n",
    "\n",
    "            if save_ckpt_fn is not None and float(test_f1s[-1]) > best_f1:\n",
    "                save_ckpt_fn()\n",
    "                best_f1 = float(test_f1s[-1])\n",
    "\n",
    "            scheduler.step(epoch=e)\n",
    "\n",
    "        return train_losses, train_f1s, test_losses, test_f1s\n",
    "    \n",
    "    def train_model(self, cache_dir=None):\n",
    "        \n",
    "        model_params = copy(self.model_params)\n",
    "\n",
    "        print(f\"\\n\\n{model_params}\")\n",
    "        lr = model_params.pop(\"learning_rate\")\n",
    "        lr_decay = model_params.pop(\"learning_rate_decay\")\n",
    "        suffix_prefix_buckets = model_params.pop(\"suffix_prefix_buckets\")  # used for another model, ignore\n",
    "\n",
    "        print(\"Creating dataloaders\")\n",
    "        train_batcher, test_batcher = self.get_dataloaders(word_emb=None, graph_emb=None, suffix_prefix_buckets=suffix_prefix_buckets, cache_dir=cache_dir)\n",
    "\n",
    "        print(\"Loading pretrained model\")\n",
    "        \n",
    "        print(\"Creating model\")\n",
    "        # definition of CodeGPT2Hybrid is at https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/codebert/codebert_train.py#L21\n",
    "        model = self.hybrid_model_class(\n",
    "            self.base_model, graph_emb=None, padding_idx=0, num_classes=train_batcher.num_classes,\n",
    "            no_graph=self.no_graph\n",
    "        )\n",
    "                \n",
    "        if self.use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        trial_dir = self.get_trial_dir()  # create directory for saving checkpoints\n",
    "        os.mkdir(trial_dir)\n",
    "        self.create_summary_writer(trial_dir)\n",
    "        \n",
    "        train_batcher.tagmap.save(os.path.join(trial_dir, \"tagmap.json\"))\n",
    "        # pickle.dump(train_batcher.tagmap, open(os.path.join(trial_dir, \"tag_types.pkl\"), \"wb\"))\n",
    "\n",
    "        def save_ckpt_fn():\n",
    "            checkpoint_path = os.path.join(trial_dir, \"checkpoint\")\n",
    "            torch.save(model, open(checkpoint_path, 'wb'))\n",
    "\n",
    "        print(\"Begin training\")\n",
    "        train_losses, train_f1, test_losses, test_f1 = self.train(\n",
    "            model=model, train_batches=train_batcher, test_batches=test_batcher,\n",
    "            epochs=self.epochs, learning_rate=lr,\n",
    "            scorer=lambda pred, true: scorer(pred, true, train_batcher.tagmap, no_localization=self.no_localization),  # need to verify scoring function\n",
    "            learning_rate_decay=lr_decay, finetune=self.finetune, save_ckpt_fn=save_ckpt_fn,\n",
    "            no_localization=self.no_localization\n",
    "        )\n",
    "\n",
    "        metadata = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"learning_rate_decay\": lr_decay,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"suffix_prefix_buckets\": suffix_prefix_buckets,\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"no_localization\": self.no_localization\n",
    "        }\n",
    "\n",
    "        print(\"Maximum f1:\", max(test_f1))\n",
    "\n",
    "        metadata.update(model_params)\n",
    "\n",
    "        with open(os.path.join(trial_dir, \"params.json\"), \"w\") as metadata_sink:\n",
    "            metadata_sink.write(json.dumps(metadata, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "All training options are specified [here](https://github.com/VitalyRomanov/method-embedding/blob/e995477db13a13875cca54c37d4d29f63b0c8e93/SourceCodeTools/nlp/entity/type_prediction.py#L256)\n",
    "Option names are added to `args` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = \"variable_misuse_graph_2_percent_balanced\"\n",
    "\n",
    "args = Namespace()\n",
    "args.__dict__.update({\n",
    "    \"learning_rate\": 1e-3,           #\n",
    "    \"max_seq_len\": 512,              # default for BERT\n",
    "    \"random_seed\": 42,               #\n",
    "    \"epochs\": 10,                   #\n",
    "    \"gpu\": -1,                       # set this to GPU id to use gpu\n",
    "    \"batch_size\": 8,                 # higher value increases memory consumption\n",
    "    \"finetune\": True,  # set this flag to enable finetuning\n",
    "    \"no_localization\": False,        # whether to solve variable misuse with, or without localization\n",
    "    \n",
    "    # do not change items below\n",
    "    \"no_graph\": True,                # used for another model\n",
    "    \"model_output\": dataset_path,    # where to store checkpoints\n",
    "    \"graph_emb_path\": None,          # used for another model\n",
    "    \"word_emb_path\": None,           # used for another model\n",
    "    \"trials\": 1,                     # setting > 1 repeats training, used to accumulate statisitcs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = read_data(dataset_path, \"train\")\n",
    "test_data = read_data(dataset_path, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_data[0]  # ignore `replacements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/CodeGPT-small-py were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/bl7awy/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.2 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = GPT2Model.from_pretrained(\"microsoft/CodeGPT-small-py\")\n",
    "base_model_tokenizer = GPT2Tokenizer.from_pretrained(\"microsoft/CodeGPT-small-py\")\n",
    "model_name = \"codegpt2\"\n",
    "hybrid_model_class = CodeGPT2HybridModel\n",
    "\n",
    "trainer = VariableMisuseDetector(\n",
    "    hybrid_model_class, model_name, base_model_tokenizer, base_model, train_data, test_data,\n",
    "    params={\"learning_rate\": 1e-4, \"learning_rate_decay\": 0.99, \"suffix_prefix_buckets\": 1},\n",
    "    graph_emb_path=args.graph_emb_path, word_emb_path=args.word_emb_path,\n",
    "    output_dir=args.model_output, epochs=args.epochs, batch_size=args.batch_size, gpu_id=args.gpu,\n",
    "    finetune=args.finetune, trials=args.trials, seq_len=args.max_seq_len, no_localization=args.no_localization,\n",
    "    no_graph=args.no_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'learning_rate': 0.0001, 'learning_rate_decay': 0.99, 'suffix_prefix_buckets': 1}\n",
      "Creating dataloaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess functions:   1%|                  | 44/7012 [00:00<00:15, 439.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created cache directory at location variable_misuse_graph_2_percent_balanced/__cache__/PythonBatcher220049262397976649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess functions: 100%|████████████████| 7012/7012 [00:13<00:00, 515.88it/s]\n",
      "Preprocess functions:   7%|█▍                 | 54/732 [00:00<00:01, 538.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created cache directory at location variable_misuse_graph_2_percent_balanced/__cache__/PythonBatcher220049262397976649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess functions: 100%|██████████████████| 732/732 [00:01<00:00, 560.00it/s]\n",
      "  0%|                                           | 1/791 [00:00<02:14,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Creating model\n",
      "Begin training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▋                                     | 88/791 [00:25<03:19,  3.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__cache__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mVariableMisuseDetector.train_model\u001b[0;34m(self, cache_dir)\u001b[0m\n\u001b[1;32m    144\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;28mopen\u001b[39m(checkpoint_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBegin training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m train_losses, train_f1, test_losses, test_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_batcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtagmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_localization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_localization\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# need to verify scoring function\u001b[39;49;00m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_ckpt_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_ckpt_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_localization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_localization\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_losses,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_f1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_localization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_localization\n\u001b[1;32m    167\u001b[0m }\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum f1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmax\u001b[39m(test_f1))\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mVariableMisuseDetector.train\u001b[0;34m(self, model, train_batches, test_batches, epochs, report_every, scorer, learning_rate, learning_rate_decay, finetune, summary_writer, save_ckpt_fn, no_localization)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_batches)):\n\u001b[1;32m     45\u001b[0m     batch_to_torch(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# inspect the content of `batch`\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     loss, p, r, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_finetune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtok_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# keep this None\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep this None\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinetune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# finetuning starts after 20% of training is complete\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_mapping\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     56\u001b[0m     ps\u001b[38;5;241m.\u001b[39mappend(p)\n",
      "File \u001b[0;32m~/Workspace/method-embedding/SourceCodeTools/nlp/codebert/codebert_train.py:149\u001b[0m, in \u001b[0;36mtrain_step_finetune\u001b[0;34m(model, optimizer, token_ids, prefix, suffix, graph_ids, labels, lengths, extra_mask, class_weights, scorer, finetune, vocab_mapping)\u001b[0m\n\u001b[1;32m    147\u001b[0m token_ids[token_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_mapping)] \u001b[38;5;241m=\u001b[39m vocab_mapping[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    148\u001b[0m seq_mask \u001b[38;5;241m=\u001b[39m get_length_mask(token_ids, lengths)\n\u001b[0;32m--> 149\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(logits, labels, mask\u001b[38;5;241m=\u001b[39mseq_mask, class_weights\u001b[38;5;241m=\u001b[39mclass_weights, extra_mask\u001b[38;5;241m=\u001b[39mextra_mask)\n\u001b[1;32m    151\u001b[0m p, r, f1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(logits, labels, mask\u001b[38;5;241m=\u001b[39mseq_mask, scorer\u001b[38;5;241m=\u001b[39mscorer, extra_mask\u001b[38;5;241m=\u001b[39mextra_mask)\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Workspace/method-embedding/SourceCodeTools/nlp/codebert/codebert_train.py:110\u001b[0m, in \u001b[0;36mCodeGPT2HybridModel.forward\u001b[0;34m(self, token_ids, graph_ids, mask, finetune)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 110\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_graph:\n\u001b[1;32m    113\u001b[0m     graph_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_emb(graph_ids)\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:891\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    881\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    882\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    883\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    888\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 891\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:391\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    389\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    390\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 391\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:313\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    311\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m     query, key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    315\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    316\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SourceCodeTools/lib/python3.8/site-packages/transformers/pytorch_utils.py:107\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    106\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train_model(cache_dir=Path(dataset_path).joinpath(\"__cache__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- CodeBert ([Huggingface](https://huggingface.co/microsoft/codebert-base))\n",
    "- CodeGPT-2 ([Huggingface](https://huggingface.co/microsoft/CodeGPT-small-py))\n",
    "- GraphCodeBert ([Huggingface](https://huggingface.co/microsoft/graphcodebert-base), [GitHub](https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT/refinement))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check example in the script\n",
    "\n",
    "`SourceCodeTools/nlp/entity/utils/visualize_dataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
